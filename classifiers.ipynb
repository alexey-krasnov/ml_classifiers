{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2ee52f-f3e1-46af-a431-b759433a89f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\"\"\"Evaluation performance of the most popular classification algorithms with\n",
    "    artificial dataset generated by sklearn.datasets.make_classification()\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2a7f8a-0148-4f4c-ad61-5724241fc264",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the dataset: X.shape is (1000, 10); y.shape is (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Define test classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=2, random_state=12)\n",
    "print(f\"Summarize the dataset: X.shape is {X.shape}; y.shape is {y.shape}\")\n",
    "# Split for train/test data as 80% and 20 %\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7eb8c9-7795-4733-b068-fc349e12e251",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(LogisticRegression(),\n RandomForestClassifier(),\n GradientBoostingClassifier(),\n HistGradientBoostingClassifier(),\n XGBClassifier(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=None, colsample_bynode=None,\n               colsample_bytree=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, gamma=None,\n               gpu_id=None, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=None, max_bin=None,\n               max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n               max_leaves=None, min_child_weight=None, missing=nan,\n               monotone_constraints=None, n_estimators=100, n_jobs=None,\n               num_parallel_tree=None, predictor=None, random_state=None,\n               reg_alpha=None, reg_lambda=None, ...),\n LGBMClassifier(),\n <catboost.core.CatBoostClassifier at 0x157bdff70>,\n MLPClassifier(max_iter=1500))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instances of the classification models\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "hist_gb = HistGradientBoostingClassifier()\n",
    "xgb = XGBClassifier()\n",
    "lgb = LGBMClassifier()\n",
    "catgb = CatBoostClassifier(verbose=0, n_estimators=200)\n",
    "mlp = MLPClassifier(max_iter=1500, solver='adam')\n",
    "\n",
    "models = (lr, rf, gb, hist_gb, xgb, lgb, catgb, mlp)\n",
    "models_name = ('LogisticRegression',\n",
    "                'RandomForestClassifier',\n",
    "                'GradientBoostingClassifier',\n",
    "                'HistGradientBoostingClassifier',\n",
    "                'XGBClassifier',\n",
    "                'LGBMClassifier',\n",
    "                'CatBoostClassifier',\n",
    "                'MLPClassifier',\n",
    "               )\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f99d79b-e97b-47b9-8b2a-f7619d37bb2d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_cross_validation(estimator, X_train, y_train, cv=5):\n",
    "    \"\"\"Perform cross validation and return metrics:\n",
    "    accuracy, balanced_accuracy, f1, precision, recall, roc_auc\"\"\"\n",
    "    scorings = ('accuracy', 'balanced_accuracy', 'f1', 'precision', 'recall', 'roc_auc')\n",
    "    scores = cross_validate(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring=scorings)\n",
    "    final_metrics = dict()\n",
    "    for key, item in sorted(scores.items()):\n",
    "        final_metrics[key] = item.mean().round(4)\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c2587e-11e6-4c35-9017-72039ff52aa7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                fit_time  score_time  test_accuracy  \\\nLogisticRegression                0.0147      0.0085         0.8325   \nRandomForestClassifier            0.7780      0.0864         0.8982   \nGradientBoostingClassifier        0.7496      0.0156         0.8988   \nHistGradientBoostingClassifier    0.5964      0.0298         0.9278   \nXGBClassifier                     0.4513      0.0269         0.9225   \nLGBMClassifier                    0.2448      0.0267         0.9238   \nCatBoostClassifier                2.4718      0.0103         0.9302   \nMLPClassifier                     7.2410      0.0119         0.9322   \n\n                                test_balanced_accuracy  test_f1  \\\nLogisticRegression                              0.8323   0.8362   \nRandomForestClassifier                          0.8982   0.9001   \nGradientBoostingClassifier                      0.8986   0.9009   \nHistGradientBoostingClassifier                  0.9278   0.9285   \nXGBClassifier                                   0.9224   0.9240   \nLGBMClassifier                                  0.9238   0.9247   \nCatBoostClassifier                              0.9301   0.9317   \nMLPClassifier                                   0.9322   0.9335   \n\n                                test_precision  test_recall  test_roc_auc  \nLogisticRegression                      0.8291       0.8449        0.9049  \nRandomForestClassifier                  0.8962       0.9047        0.9672  \nGradientBoostingClassifier              0.8946       0.9081        0.9582  \nHistGradientBoostingClassifier          0.9309       0.9269        0.9721  \nXGBClassifier                           0.9198       0.9289        0.9711  \nLGBMClassifier                          0.9264       0.9235        0.9711  \nCatBoostClassifier                      0.9247       0.9393        0.9745  \nMLPClassifier                           0.9298       0.9378        0.9761  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fit_time</th>\n      <th>score_time</th>\n      <th>test_accuracy</th>\n      <th>test_balanced_accuracy</th>\n      <th>test_f1</th>\n      <th>test_precision</th>\n      <th>test_recall</th>\n      <th>test_roc_auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LogisticRegression</th>\n      <td>0.0147</td>\n      <td>0.0085</td>\n      <td>0.8325</td>\n      <td>0.8323</td>\n      <td>0.8362</td>\n      <td>0.8291</td>\n      <td>0.8449</td>\n      <td>0.9049</td>\n    </tr>\n    <tr>\n      <th>RandomForestClassifier</th>\n      <td>0.7780</td>\n      <td>0.0864</td>\n      <td>0.8982</td>\n      <td>0.8982</td>\n      <td>0.9001</td>\n      <td>0.8962</td>\n      <td>0.9047</td>\n      <td>0.9672</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingClassifier</th>\n      <td>0.7496</td>\n      <td>0.0156</td>\n      <td>0.8988</td>\n      <td>0.8986</td>\n      <td>0.9009</td>\n      <td>0.8946</td>\n      <td>0.9081</td>\n      <td>0.9582</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingClassifier</th>\n      <td>0.5964</td>\n      <td>0.0298</td>\n      <td>0.9278</td>\n      <td>0.9278</td>\n      <td>0.9285</td>\n      <td>0.9309</td>\n      <td>0.9269</td>\n      <td>0.9721</td>\n    </tr>\n    <tr>\n      <th>XGBClassifier</th>\n      <td>0.4513</td>\n      <td>0.0269</td>\n      <td>0.9225</td>\n      <td>0.9224</td>\n      <td>0.9240</td>\n      <td>0.9198</td>\n      <td>0.9289</td>\n      <td>0.9711</td>\n    </tr>\n    <tr>\n      <th>LGBMClassifier</th>\n      <td>0.2448</td>\n      <td>0.0267</td>\n      <td>0.9238</td>\n      <td>0.9238</td>\n      <td>0.9247</td>\n      <td>0.9264</td>\n      <td>0.9235</td>\n      <td>0.9711</td>\n    </tr>\n    <tr>\n      <th>CatBoostClassifier</th>\n      <td>2.4718</td>\n      <td>0.0103</td>\n      <td>0.9302</td>\n      <td>0.9301</td>\n      <td>0.9317</td>\n      <td>0.9247</td>\n      <td>0.9393</td>\n      <td>0.9745</td>\n    </tr>\n    <tr>\n      <th>MLPClassifier</th>\n      <td>7.2410</td>\n      <td>0.0119</td>\n      <td>0.9322</td>\n      <td>0.9322</td>\n      <td>0.9335</td>\n      <td>0.9298</td>\n      <td>0.9378</td>\n      <td>0.9761</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=12)\n",
    "data_cross_val = []\n",
    "for mdl in models:\n",
    "    report_dict = make_cross_validation(mdl, X_train, y_train, cv=cv)\n",
    "    data_cross_val.append(report_dict)\n",
    "df_report = pd.DataFrame(data_cross_val, index=models_name)\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dff69a9-2000-4b64-adbd-435dc1fb8f23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_evaluation(estimator, X_train, y_train, X_test, y_test) -> dict:\n",
    "    \"\"\"Calculate the main model metrics - accuracy, balanced-Accuracy,\n",
    "    recall, precision, f1_score on the test set and return them as a map object.\"\"\"\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred).round(4)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_pred).round(4)\n",
    "    precision = metrics.precision_score(y_test, y_pred).round(4)\n",
    "    f_1 = metrics.f1_score(y_test, y_pred).round(4)\n",
    "    recall = metrics.recall_score(y_test, y_pred).round(4)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_pred).round(4)\n",
    "\n",
    "    final_metrics = {'accuracy': accuracy,\n",
    "                     'balanced_accuracy': balanced_accuracy,\n",
    "                     'f1': f_1,\n",
    "                     'precision': precision,\n",
    "                     'recall': recall,\n",
    "                     'roc_auc': roc_auc,\n",
    "                    }\n",
    "    # Use dict comprehension for round dict values\n",
    "    final_metrics = {key: round(val, 4) for key, val in final_metrics.items()}\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e31f28-5fbe-44d1-8bd3-f6a33277192d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the models on the test set: \n"
     ]
    },
    {
     "data": {
      "text/plain": "                                accuracy  balanced_accuracy      f1  \\\nLogisticRegression                 0.845             0.8438  0.8324   \nRandomForestClassifier             0.915             0.9145  0.9110   \nGradientBoostingClassifier         0.890             0.8898  0.8866   \nHistGradientBoostingClassifier     0.940             0.9400  0.9388   \nXGBClassifier                      0.920             0.9196  0.9167   \nLGBMClassifier                     0.930             0.9300  0.9286   \nCatBoostClassifier                 0.950             0.9498  0.9485   \nMLPClassifier                      0.945             0.9449  0.9436   \n\n                                precision  recall  roc_auc  \nLogisticRegression                 0.8851  0.7857   0.8438  \nRandomForestClassifier             0.9355  0.8878   0.9145  \nGradientBoostingClassifier         0.8958  0.8776   0.8898  \nHistGradientBoostingClassifier     0.9388  0.9388   0.9400  \nXGBClassifier                      0.9362  0.8980   0.9196  \nLGBMClassifier                     0.9286  0.9286   0.9300  \nCatBoostClassifier                 0.9583  0.9388   0.9498  \nMLPClassifier                      0.9485  0.9388   0.9449  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accuracy</th>\n      <th>balanced_accuracy</th>\n      <th>f1</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>roc_auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LogisticRegression</th>\n      <td>0.845</td>\n      <td>0.8438</td>\n      <td>0.8324</td>\n      <td>0.8851</td>\n      <td>0.7857</td>\n      <td>0.8438</td>\n    </tr>\n    <tr>\n      <th>RandomForestClassifier</th>\n      <td>0.915</td>\n      <td>0.9145</td>\n      <td>0.9110</td>\n      <td>0.9355</td>\n      <td>0.8878</td>\n      <td>0.9145</td>\n    </tr>\n    <tr>\n      <th>GradientBoostingClassifier</th>\n      <td>0.890</td>\n      <td>0.8898</td>\n      <td>0.8866</td>\n      <td>0.8958</td>\n      <td>0.8776</td>\n      <td>0.8898</td>\n    </tr>\n    <tr>\n      <th>HistGradientBoostingClassifier</th>\n      <td>0.940</td>\n      <td>0.9400</td>\n      <td>0.9388</td>\n      <td>0.9388</td>\n      <td>0.9388</td>\n      <td>0.9400</td>\n    </tr>\n    <tr>\n      <th>XGBClassifier</th>\n      <td>0.920</td>\n      <td>0.9196</td>\n      <td>0.9167</td>\n      <td>0.9362</td>\n      <td>0.8980</td>\n      <td>0.9196</td>\n    </tr>\n    <tr>\n      <th>LGBMClassifier</th>\n      <td>0.930</td>\n      <td>0.9300</td>\n      <td>0.9286</td>\n      <td>0.9286</td>\n      <td>0.9286</td>\n      <td>0.9300</td>\n    </tr>\n    <tr>\n      <th>CatBoostClassifier</th>\n      <td>0.950</td>\n      <td>0.9498</td>\n      <td>0.9485</td>\n      <td>0.9583</td>\n      <td>0.9388</td>\n      <td>0.9498</td>\n    </tr>\n    <tr>\n      <th>MLPClassifier</th>\n      <td>0.945</td>\n      <td>0.9449</td>\n      <td>0.9436</td>\n      <td>0.9485</td>\n      <td>0.9388</td>\n      <td>0.9449</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the models on the test set\n",
    "data_test = []\n",
    "for mdl in models:\n",
    "    report_dict_test = test_evaluation(mdl, X_train, y_train, X_test, y_test)\n",
    "    data_test.append(report_dict_test)\n",
    "print('Evaluate the models on the test set: ')\n",
    "df_report_test = pd.DataFrame(data_test, index=models_name)\n",
    "df_report_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}